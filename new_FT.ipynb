{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', ' result'],\n",
       "        num_rows: 8\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', ' result'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "\n",
    "dataset = load_dataset(\"Ankita802/formatted-data\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'AS a CONNECT developer, I want all assertions upgrades to be completely tested so the code can be included in the next release',\n",
       " ' result': 'The CONNECT developerï¿½s objective is to thoroughly test all assertion upgrades to ensure they are ready for inclusion in the upcoming release. This comprehensive testing is crucial for verifying that the new enhancements function correctly and meet quality standards, thereby contributing to the stability and reliability of the next version of the software.'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': Value(dtype='string', id=None), ' result': Value(dtype='string', id=None)}\n",
      "{'input': 'AS a CONNECT developer, I want all assertions upgrades to be completely tested so the code can be included in the next release', ' result': 'The CONNECT developerï¿½s objective is to thoroughly test all assertion upgrades to ensure they are ready for inclusion in the upcoming release. This comprehensive testing is crucial for verifying that the new enhancements function correctly and meet quality standards, thereby contributing to the stability and reliability of the next version of the software.'}\n",
      "{'input': 'As a CONNECT adopter I want Release Notes for 4.3 to be documented on the wiki, so that I understand the features, bug fixes packaged with this release', ' result': 'The Release Notes for CONNECT 4.3 are documented on the wiki, detailing new features, bug fixes, and improvements. This includes enhancements to the Setup Wizard, updates to the core codebase, and security improvements. The documentation helps adopters understand the changes and how they impact their implementation.'}\n",
      "{'input': 'As a Developer, I want to prevent duplicate transactions from being published and deal with the time gap between validation and the publishing decision.', ' result': 'The developer aims to implement a mechanism to prevent the publication of duplicate transactions and address the time gap between validation and the publishing decision. This involves creating a system that identifies and filters out duplicate transactions before they are published, ensuring data integrity and reducing redundancy. Additionally, strategies will be devised to manage the time delay between validation and publishing, possibly through automated workflows or notification systems to streamline the process and ensure timely decision-making.'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'].features)\n",
    "print(dataset['train'][0])\n",
    "print(dataset['train'][5])\n",
    "print(dataset['train'][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "\n",
    "model_name = 'roberta-base'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "model = RobertaModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 124645632\n",
      "all model parameters: 124645632\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: [0, 2336, 10, 8748, 42849, 6596, 6, 38, 236, 70, 29947, 11500, 7, 28, 2198, 4776, 98, 5, 3260, 64, 28, 1165, 11, 5, 220, 800, 2]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Decoded Input: <s>AS a CONNECT developer, I want all assertions upgrades to be completely tested so the code can be included in the next release</s>\n"
     ]
    }
   ],
   "source": [
    "sentence = \"AS a CONNECT developer, I want all assertions upgrades to be completely tested so the code can be included in the next release\"\n",
    "tokenized_input = tokenizer(sentence)\n",
    "# Print the tokenized input\n",
    "print(\"Input IDs:\", tokenized_input[\"input_ids\"])\n",
    "# print(\"Token Type IDs:\", tokenized_input[\"token_type_ids\"])\n",
    "print(\"Attention Mask:\", tokenized_input[\"attention_mask\"])\n",
    "\n",
    "# Decode the input tokens\n",
    "decoded_input = tokenizer.decode(tokenized_input[\"input_ids\"])\n",
    "print(\"Decoded Input:\", decoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.dataset_dict.DatasetDict'>\n",
      "{'input': 'AS a CONNECT developer, I want all assertions upgrades to be completely tested so the code can be included in the next release', ' result': 'The CONNECT developerï¿½s objective is to thoroughly test all assertion upgrades to ensure they are ready for inclusion in the upcoming release. This comprehensive testing is crucial for verifying that the new enhancements function correctly and meet quality standards, thereby contributing to the stability and reliability of the next version of the software.'}\n",
      "{'input': 'As a Publisher, I would like a tool to check data availability persistence after publication.', ' result': 'As a Publisher, I need a tool to verify the persistence of data availability after publication.\" This user story succinctly conveys the requirement for a tool to ensure that published data remains available and accessible over time, addressing the concerns of data persistence.'}\n",
      "{'input': 'As a administrator, I want to refund sponsorship money that was processed via stripe, so that people get their monies back.', ' result': \"The administrator's goal is to efficiently refund sponsorship payments processed through Stripe, ensuring users promptly receive their funds. By initiating refunds via Stripe, the administrator facilitates a seamless reimbursement process, providing users with a hassle-free experience. This ensures transparency and trust, as users can rely on timely refunds for any sponsorship payments made.\"}\n"
     ]
    }
   ],
   "source": [
    "# Print the type of the dataset\n",
    "print(type(dataset))\n",
    "\n",
    "# Print the first few entries of the dataset\n",
    "for i in range(3):\n",
    "    print(dataset['train'][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: AS a CONNECT developer, I want all assertions upgrades to be completely tested so the code can be included in the next release\n",
      "Result: The CONNECT developerï¿½s objective is to thoroughly test all assertion upgrades to ensure they are ready for inclusion in the upcoming release. This comprehensive testing is crucial for verifying that the new enhancements function correctly and meet quality standards, thereby contributing to the stability and reliability of the next version of the software.\n",
      "\n",
      "Input: As a Publisher, I would like a tool to check data availability persistence after publication.\n",
      "Result: As a Publisher, I need a tool to verify the persistence of data availability after publication.\" This user story succinctly conveys the requirement for a tool to ensure that published data remains available and accessible over time, addressing the concerns of data persistence.\n",
      "\n",
      "Input: As a administrator, I want to refund sponsorship money that was processed via stripe, so that people get their monies back.\n",
      "Result: The administrator's goal is to efficiently refund sponsorship payments processed through Stripe, ensuring users promptly receive their funds. By initiating refunds via Stripe, the administrator facilitates a seamless reimbursement process, providing users with a hassle-free experience. This ensures transparency and trust, as users can rely on timely refunds for any sponsorship payments made.\n",
      "\n",
      "Input: As a Researcher, I want to be able to save new visualizations, so that I can share them with others or include them in the Data Package.\n",
      "Result: To save new visualizations as a researcher, create them using tools like R, Python, or Tableau, then export them in common formats like PNG or PDF. Store them in a designated folder and share them with others or include them in the Data Package documentation for wider use.\n",
      "\n",
      "Input: As a Data Consuming User, I want to see textual descriptions that accompany embedded visualisations, So that I can more easily understand what I am viewing.\n",
      "Result: As a Data Consuming User, I seek textual descriptions accompanying embedded visualizations to enhance my understanding of the content. These descriptions provide valuable context and explanations that aid in comprehending the data presented, ensuring clarity and facilitating informed interpretation of the visual information.\n",
      "\n",
      "Input: As a CONNECT adopter I want Release Notes for 4.3 to be documented on the wiki, so that I understand the features, bug fixes packaged with this release\n",
      "Result: The Release Notes for CONNECT 4.3 are documented on the wiki, detailing new features, bug fixes, and improvements. This includes enhancements to the Setup Wizard, updates to the core codebase, and security improvements. The documentation helps adopters understand the changes and how they impact their implementation.\n",
      "\n",
      "Input: As a Developer, I want to prevent duplicate transactions from being published and deal with the time gap between validation and the publishing decision.\n",
      "Result: The developer aims to implement a mechanism to prevent the publication of duplicate transactions and address the time gap between validation and the publishing decision. This involves creating a system that identifies and filters out duplicate transactions before they are published, ensuring data integrity and reducing redundancy. Additionally, strategies will be devised to manage the time delay between validation and publishing, possibly through automated workflows or notification systems to streamline the process and ensure timely decision-making.\n",
      "\n",
      "Input: As a Custodial Agent we need to reduce the amount of up-front policy and planning for security requirements and what is assigned to the implementers, this should be largely predetermined for all implementers and simplified.\n",
      "Result: Establish a standardized security framework that encompasses essential security requirements applicable to all implementations. This framework should cover key areas such as access control, data encryption, authentication mechanisms, and logging practices. Preconfigured Security Profiles: Develop preconfigured security profiles or templates tailored to different implementation scenarios. These profiles should encapsulate predefined security settings aligned with industry best practices and regulatory standards. Implementers can select the appropriate profile based on their deployment needs, reducing the need for manual configuration. Automated Security Setup: Implement automation tools and scripts to streamline the setup and configuration of security measures. Automate routine tasks such as user provisioning, access control policy enforcement, and security policy enforcement to minimize manual intervention and potential errors. Comprehensive Documentation: Provide comprehensive documentation outlining the predefined security requirements, recommended configurations, and deployment guidelines. This documentation should serve as a reference for implementers, offering clear instructions on how to meet security standards without extensive planning or policy formulation. Training and Support: Offer training sessions, workshops, or online resources to educate implementers on security best practices and the use of predefined security profiles. Ensure that implementers have access to support channels where they can seek assistance or clarification regarding security-related queries. Continuous Compliance Monitoring: Implement mechanisms for continuous compliance monitoring to ensure adherence to predefined security standards post-deployment. Utilize automated monitoring tools and periodic audits to identify deviations from established security policies and address them promptly. Feedback Mechanism: Establish a feedback mechanism to gather input from implementers regarding the effectiveness and usability of predefined security measures. Use this feedback to iteratively refine security profiles and streamline the implementation process further. Collaboration with Stakeholders: Collaborate closely with stakeholders, including security experts, regulatory bodies, and industry partners, to validate and refine the predefined security framework. Engage in ongoing discussions to align security requirements with evolving threats and emerging technologies.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the examples in the dataset and print the \"input\" and \"result\" columns\n",
    "for example in dataset['train']:\n",
    "    input_text = example['input']\n",
    "    result_text = example[' result']  # Note the leading space in the column name\n",
    "    print(\"Input:\", input_text)\n",
    "    print(\"Result:\", result_text)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input', ' result'])\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "example = dataset['test'][index]\n",
    "print(example.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model with zero shot inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'input_ids': tensor([[    0, 13786,  8231,     5,  8194,   287,    10,  3018,     6,    38,\n",
      "           236,     7, 22785,  1061,  1412,    11, 28132, 23730,    19,    10,\n",
      "          3748,    12,   805, 29419,   215,    25,  1204, 29419,     4,  1437,\n",
      "             2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1]])}\n",
      "\n",
      " Providing the description As a user, I want to sync events created in NeuroHub with a web-based Calendar such as Google Calendar. \n",
      "\n",
      "Example  3\n",
      "INPUT PROMPT:\n",
      " Providing the description As a user, I want to sync events created in NeuroHub with a web-based Calendar such as Google Calendar. \n",
      "\n",
      "ANSWER FROM CSV:\n",
      "Users seek the capability to synchronize events created within NeuroHub with a web-based calendar service like Google Calendar, facilitating seamless integration and access to scheduling information across platforms.\n",
      "\n",
      "MODEL GENERATION - WITH ONE SHOT LEARNING:\n",
      " Providing the description As a user, I want to sync events created in NeuroHub with a web-based Calendar such as Google Calendar. \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# example_indices = [0, 1]\n",
    "\n",
    "index = 0\n",
    "\n",
    "# for i, index in enumerate(example_indices):\n",
    "dialogue = dataset['test'][index]['input']\n",
    "summary = dataset['test'][index][' result']\n",
    "\n",
    "prompt_template = f\"\"\" Providing the description {dialogue} \"\"\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
    "\n",
    "inputs = tokenizer(prompt_template, return_tensors='pt')\n",
    "\n",
    "decoded_input = tokenizer.decode(\n",
    "            inputs['input_ids'][0],\n",
    "            skip_special_tokens=True)\n",
    "    \n",
    "print()\n",
    "print(inputs)\n",
    "print()\n",
    "print(decoded_input)\n",
    "print()\n",
    "\n",
    "# print(dash_line)\n",
    "print('Example ', i + 1)\n",
    "    # print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt_template}')\n",
    "print()\n",
    "    # print(dash_line)\n",
    "print(f'ANSWER FROM CSV:\\n{summary}')\n",
    "print()\n",
    "    # print(dash_line)\n",
    "print(f'MODEL GENERATION - WITH ONE SHOT LEARNING:\\n{decoded_input}\\n')\n",
    "print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing Full Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"input\"]]\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\" result\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', ' result', 'input_ids', 'labels'],\n",
       "        num_rows: 8\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', ' result', 'input_ids', 'labels'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns(['input', ' result'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (8, 2)\n",
      "Test: (2, 2)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 8\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "# print(f\"Validation: {tokenized_datasets['test'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(8))\n",
    "# small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(small_train_dataset)\n",
    "# print(small_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "output_dir = f'./code-generation-training-{str(int(time.time()))}'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    # logging_steps=500,\n",
    "    max_steps=-1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236b98c59d6f4fca85ba77446c68dbf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"test-squad-trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluationModule(name: \"accuracy\", module_type: \"metric\", features: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}, usage: \"\"\"\n",
      "Args:\n",
      "    predictions (`list` of `int`): Predicted labels.\n",
      "    references (`list` of `int`): Ground truth labels.\n",
      "    normalize (`boolean`): If set to False, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples. Defaults to True.\n",
      "    sample_weight (`list` of `float`): Sample weights Defaults to None.\n",
      "\n",
      "Returns:\n",
      "    accuracy (`float` or `int`): Accuracy score. Minimum possible value is 0. Maximum possible value is 1.0, or the number of examples input, if `normalize` is set to `True`.. A higher score means higher accuracy.\n",
      "\n",
      "Examples:\n",
      "\n",
      "    Example 1-A simple example\n",
      "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
      "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])\n",
      "        >>> print(results)\n",
      "        {'accuracy': 0.5}\n",
      "\n",
      "    Example 2-The same as Example 1, except with `normalize` set to `False`.\n",
      "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
      "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)\n",
      "        >>> print(results)\n",
      "        {'accuracy': 3.0}\n",
      "\n",
      "    Example 3-The same as Example 1, except with `sample_weight` set.\n",
      "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
      "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], sample_weight=[0.5, 2, 0.7, 0.5, 9, 0.4])\n",
      "        >>> print(results)\n",
      "        {'accuracy': 0.8778625954198473}\n",
      "\"\"\", stored examples: 0)\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import evaluate\n",
    "\n",
    "# metric = evaluate.load(\"accuracy\")\n",
    "# print(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import RobertaModel\n",
    "\n",
    "# model_name = 'roberta-base'\n",
    "# model = RobertaModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# output_dir = f'./checkpoints2-{str(int(time.time()))}'\n",
    "\n",
    "# from transformers import TrainingArguments\n",
    "\n",
    "# # training_args = TrainingArguments(output_dir=output_dir, \n",
    "# #                                   prediction_loss_only=bool,\n",
    "# #                                   per_device_train_batch_size=8, \n",
    "# #                                   per_device_eval_batch_size=8,\n",
    "# #                                   learning_rate=5e-5, \n",
    "# #                                   evaluation_strategy=\"epoch\", \n",
    "# #                                   logging_dir=\"logs\")\n",
    "\n",
    "# training_args = TrainingArguments(output_dir=output_dir, \n",
    "#                                   evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "# from transformers import default_data_collator\n",
    "\n",
    "# data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import Trainer\n",
    "\n",
    "# # Pass other arguments to Trainer as before\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_datasets['train'],\n",
    "#     eval_dataset=tokenized_datasets['test'],\n",
    "#     data_collator=data_collator,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d48a9bc0ec754a5dbb6544471c4f30bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model(\"test-squad-trained\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
